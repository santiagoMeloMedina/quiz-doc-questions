Section 3. Getting started with AWS

How to choose the application’s region?
In order to choose the region for a project, you must consider compliance, proximity, availability of services and pricing. Compliance refers to meeting data and government requirements, proximity stands for the applications users distance for the region. Not all regions have all services available, for example new services. Also pricing varies from region to region.

What are Availability Zones (AZ)?
They’re data centers separated from each other, isolated from disasters but are connected with high bandwidth and ultra low latency in a region. A region usually has 3 AZs.

How many permissions should one grant a user?
Following the least privilege principle, you should give only the necessary permissions.


Section 4. IAM & AWS CLI

How can I protect a user or account?
You can create password policies for requirements on password creation or updation of password, and also multifactor authentication (MFA), you can use this by different devices, either a virtual MFA device, universal 2nd factor U2F security key, hardware key fob MFA, or AWS GovCloud hardware key fob MFA.

How do AWS services perform actions on your behalf?
You need to create IAM roles and assign them to the resources that will perform the actions, for example adding one to an EC2 Instance to access AWS.

How can I make my IAM use more secure?
You can use the IAM security tools, which are report generation by credential reports at the account level on which you can see information about the user credentials like last accesses and status. And access advisor at the user level where you can see information about their permissions and access to services.

What IAM best practices?
Create strong password policy
Use and enforce MFA
One user = One physical user
Don't use root AWS Account
Never share access keys

How can you control the spending of an AWS account?
You can create a budget with respective alerts that can call different actions and can be triggered by a percentage of the budget for actual cost and forecasted cost.
Section 5. EC2 Fundamental

What types of ec2 instances there are?
There’s instances specialized for different purposes, besides those we have the general purpose ones (t2, t3, t4g, m5…). Now there's computing optimized for high performance applications like gaming, machine learning, media transcoding (start with C). Also there's’ the memory optimized for cache and in memory databases performance and finally there’s the storage optimized for databases, data warehousing applications or distributed file systems.
(ec2instances.info to make comparisons).

What are some important ports to know?
SHH(22) logging into linux instance, FTP(21) transferring files to file share, SFTP(22) transfer files through SSH, HTTP(80) web application, HTTPS(443) secure web application, RDP(3389) logging into windows instance.

How do you securely connect to an ec2 instance through ssh?
You need to have a key per and have the correct permissions set to it which can be achieved with 0400.

What are some key differences between dedicated host and dedicated instance ec2?
Although both enable use of physical servers, dedicated instances are charged per instance and dedicated hosts are charged per host. Also from a dedicated host you have visibility of sockets, cores, hostID and targeted instance placement as well as automatic instance placement and can add capacity by request. Dedicated instances are in a hardware dedicated to you but it may be shared running other instances in the same account.

What kind of ec2 computing can one acquire?
There are On-demand instances intended for short term uninterrupted workloads, these are pay per use and are not as cheap as other kinds. Spot instances are intended for resilient to failure workloads since they can be turned off and are the cheapest ec2 option since they can get even 90% discount from the on-demand instances price. Dedicated hosts and Dedicated instances are intended for strong compliance needs or licensing needs since there is hardware dedicated to you and therefore is the most expensive option. Reserved instances are intended for long term workloads since they have a fixed price for a period of either one or three years, as expected they are cheaper then on-demand instances about 72% discount from them, and can be paid upfront for even more discount. Reserved Capacity are intended for short term uninterrupted workloads in the case that you want to be sure that an instance in an specific AZ with a certain duration will be available, they are charged like on-demand instances and have no discount. There’s also Saving plans intended for long term usage where you can specify a specific instance family en location and commit to a certain type of use for a period of one or three years, these will have a discount similar to the reserved instances.



How to effectively terminate spot instances?
When defining a spot instance you can request it as one time or persistence, if it is one time, then when spot instances are interrupted by current price going over maximum defined price, then nothing else happen after interruption, but when they are of type persistence, then the instances are relaunched automatically, so always be sure to first cancel spot instances and finally terminate the instances themselves.


What are some used strategies to launch spot instances with spot fleets?
You can base your strategy on pricing, availability or capacity where the fleet will choose pools with the lowest price (lowestPrice), choose instances distributed across multiple pools (diversified) or choose pools with the optimal capacity (capacityOptimized) respectively.


Section 6. Solutions Architect Associate Level

Should I use Elastic IP?
It’s not a good architectural practice, the best strategy is using a load balancer or at least using a public IP with a DNS mapped to it.

What are the differences between the ec2 placements groups strategies?
We have a strategy intended for applications that require low latency and high network throughput (Cluster) with hardware in the same rack and AZ. Applications with the requirement of high availability and critical applications with low tolerance to failure from other racks (Spread) with hardware in different racks and different AZs, with an instance limit of 7 per AZ. And big data applications (Partition) that need a limitless number of instances with up to 7 partitions on each AZ and multiple instances across different racks on a partition.

How can one boot up the instances faster after stopping and starting them again?
There is a feature called Hibernation in which the instance is stopped but it stores RAM’s data on a file on the encrypted EBS (must be encrypted), then the instance is stopped. This allows the instances to not have to boot the OS again since the RAM state is preserved.


Section 7. EC2 Instance Storage

What happens if I need more IOPS in my virtualization or a better network performance?
You can use AWS Nitro which is a new virtualization technology that can handle enhanced network, HPC and IPv6, and also can handle up 64k EBS IOPS.

What is the difference between vCPUs and CPUs?
The CPUs or cores of an instance contain threads to make use of multithreaded applications. These threads are recognized in AWS as vCPUs. And you can configure an instance vCPU options by choosing the number of cores (# cores) that will be used, this is intended for use of high RAM, the number of vCPUs per code, this is intended for a higher thread performance.

Can I use an EBS volume on an instance in another AZ from the one it was created?
Yes but only using EBS Snapshot. EBS volumes are network drives that can be attached to one instance at a time (except for some cases) and can be attached to any instance in the same AZ. But when you choose to attach a certain EBS to use its persisted data, on an instance in another AZ you need to make a backup (EBS Snapshot) and use it on the targeted AZ instance. If you are looking for a cheaper option you can move the snapshot to a snapshot archive which is 75% cheaper and can restore it after 24 to 72 hours.

Is there any way to preconfigure my instances with software efficiently?
You can create your own IAM which already has a user data configuration installed to be used faster.

What if I need better store performance for an instance than EBS?
Some instances may offer a local instance store which is physical storage hardware connected to the machine the instances are being virtualized on. These can have really high IOPS even to the millions, but the information being stored there is ephemeral since it is going to be lost at instance stop. Therefore this option is for information like buffers or one needs to have a backup for the storage.

Are there different types of EBS Volumes?
There are 6 types of EBS Volumes, there's the gp2 and gp3 which you use to boot an instance, virtual desktops or development and test environments, they are SSD and are cost effective low latency. The difference between them is that gp2 size is linked to IOPS by 3 IOPS per GB and a maximum of 16k IOPS. There’s io1 and io2 which are SSD that can also boot an instance and are intended for critical business applications like database workloads given that it needs more than 16k IOPS, here the maximum IOPS are 64k using Nitro instances or 32k without it, here we can find io2 block express with up to 256k IOPS. Finally there’s the st1 which is HDD intended for data warehousing, log processing and big data with only a maximum of 500 IOPS, and sc1 which is also HDD and is intended for lowest cost decisions like archiving files with a maximum of 250 IOPS.

Is it possible to attach EBS Volumes to multiple instances?
Yes but only with an EBS of type io1 or io2 and this is intended for applications that require higher availability like clustered linux applications or applications with concurrent write operations.

Is it possible to encrypt an EBS Volume that wasn't encrypted at creation?
Yes, you can do it by creating an encrypted EBS Snapshot from it and then creating a volume from that snapshot.

Is there another type of storage to attach to EC2 like EBS?
Yes, the EFS network file system is more scalable, and allows parallelism. This file system supports multi-instances attached at the same time and cross AZ, and it's more expensive. It’s automatically scalable so it doesnt need to plan capacity. It supports thousands of NFS connections and has a throughput up to 10GB/s. It's more expensive.

How configurable is EFS?
You can configure it according to performance and storage class. On performance you can choose between general purpose intended for serve web for example, or max I/O which has higher throughput, high parallelism but higher latency; the throughput can be chosen either burst or provisioned where burst has a 5Mb/s that can burst up to 100Mb/s per Tb, and provisioned is deterministic so it can be 1GB/s for the Tb. On storage class you can choose it to be either standard or infrequent access (IA) where in the latter is going to be much cheaper (~90%). There’s also lifetime policies that place standard class into IA class after a certain defined class automatically.

When shouldn’t one make and restore snapshots from an EBS?
When there’s too much traffic since this operation uses IOs and this can result in decreased performance.

What is the difference in pricing on EBS and EFS?
On EBS you have to provision capacity and that will determine billing. On EFS it's pay for use.


Section 8. High Availability and Scalability. ELB & ASG

How can I scale horizontally on AWS EC2 without much effort?
You can use ELB(Elastic load balancer) which receives traffic and redirects it to the different instances. It’s fundamental that instances have a /<anything or empty> endpoint that can return 200 when asked by the load balancer so this can redirect traffic to it.

Do I need to have all my load balanced instances receiving internet traffic?
No, you just need to have them receive inbound traffic from your ELB security group, and the load balancer will be the one to receive internet traffic.

What types of ELB are there?
There’s Classic(CLB) type that handles TPC Layer 7, HTTP, HTTPs, and layer4, and makes health checks via HTTP or HTTPS, this one is currently almost deprecated since it's the oldest and dont have as many features. Application(ALB) type which handles Layer 7 traffic, supports HTTP/2 and WebSockets. This one is going to load balance target groups where the health checks are at the target group level, and can route to a determined target group based on path, hostname or query strings, and also has port mapping for routing. Network (NLB) , which operates on Layer 4 for TCP and UDP, has lower latency (~300 less than ALB). Is intended for extreme performance and offers one static IP per AZ and the ability to assign an elastic IP. It routes traffic to target groups. Gateway (GWLB) operates at Layer 3 and takes traffic and redirects it to target groups with virtual third party security appliances to analyze the traffic, and if there is no problem the petition gets routed to the instances. Basically uses the functions of transparent network gateway and load balancer.

What kinds of target groups are there to be load balanced by ALB?
There’s EC2 instances that can be managed by autoscaling groups. IP addresses, ECS and lambda functions where the http request gets translated into JSON.

What kinds of target groups does NLB route traffic to?
They are EC2 instances, private IP addresses and ALBs.

How do I know a client's IP and Port from an instance using ALB?
You can check the headers X-Forwarded-For, X-Forwarded-Port and X-Forwarded-Proto.

Is it possible to route traffic to the same instance for the same users?
Yes you can do so by configuring stickiness on the target group. This feature is supported by CLB and ALB. It works by sending and identifying cookies that can be of two types, the application based cookie and the load balancer generated cookie which can be understanded as a duration based cookie.

Is there a way to balance traffic between AZs where each has their ELB?
Yes, it is called Cross Zone load balancing. When you don't have load cross zone load balancing between AZs, even if you get an equal amount of traffic for each AZ, some instances are going to be more overloaded than others if there isn't an equal number of instances on the AZs. With cross zone load balancing you distribute the traffic accordingly through all the instances in the different AZs so all of them receive the same amount of traffic. This feature is enabled by default with no cost on ALB, disable by default on CLB but can be enabled without any costs and also disabled by default on NLB but this one has enabling costs.

How can you secure the communication with the load balancer?
ELB uses X.509 SSL certificates for HTTPS. You can upload your own certificates or use ACM (Amazon certificate manager) to configure them into the ELB. CLB can only have one certificate, but ALB and NLB use SNI(Server Name Indication) which allows them to have multiple certificates for different targets.

What happens to the users connected through the ELB to an instance that is deregistering?
When an instance is deregistering or draining you can set a time value to wait for the connected users processes to finish and avoid allowing new requests but instead route to the other instances in the ELB. This value can be from 1 second to 1 hour and can also be disabled.

How can I automatically scale instances?
You can use Auto Scaling Groups (ASG) where you can scale in(less instances) or scale out (more instances) defining a minimum, maximum and desired capacity. This auto scaling can be configured to act on metrics like CPU usage, Average network in and out, request on ELB, and others. The ASG can act with cloudwatch alarms as well which are configured according to metrics. They are configured by either a launch configuration or template.

How can I configure an ASG to scale according to different scenarios?
You can use Scaling policies which come in two types, Dynamic and Predictive scaling or Schedule Actions. The first one Dynamic has three subtypes, there’s Target tracking scaling where you setup a certain metric to follow an a percentage to trigger an alarm for scaling, there’s Simple in which you define an action like adding or removing instances according to a certain cloudwatch alarm and there’s Step which is like simple but you can add multiple steps. The second one Predictive works with AI by forecasting the usage and the necessity to automatically scale over time. And the latter Schedule Actions work by setting scaling for specific date and times or time ranges.

When do the ASG initiate new actions after scaling?
You set up a cooldown period where no instances are going to be initiated or terminated after a previous execution.

How do I know which instances are going to terminate at scaling in on an ASG?
There is a default termination policy where it chooses the AZs with greater number of instances and at second criteria it chooses the oldest configured instance.

What if I want to execute specific actions when initiating or terminating an instance on the ASG?
You can specify them on the Lifecycle hooks that consist of the states that come after an execution of creation or termination. This way there are two states, pending:wait which comes after execution and pending:proceed which comes before finalizing execution.


Section 9. AWS Fundamentals. RDS + Aurora + ElasticCache.

Why would I use RDS instead of just installing a database engine in my ec2 instance?
Because RDS(Relational Database Service) offers many services around the database engine like auto provisioning, vertical and horizontal scaling, backup and restore to a specific timestamp, you can backup to EBS using gp2 or io1, also it offers monitoring dashboards, read replicas for performance improvement and multi AZ configuration.

How can you backup RDS?
You can set automatic backups which backup the database daily and also backup logs every 5 minutes. You can also use DB Snapshots which can be retained indefinitely different from the automatic backups which can be retained from 0 to 35 days. There’s also the option to backup on a specific window frame.


Can you automatically scale RDS?
Yes, you can use AutoScaling to scale depending on the free storage of the database. There’s conditions for scaling when it’s enabled, which are that free storage is at 10%, it hasn't been executed in 6 hours and the low storage has been maintained for 5 minutes. For scaling there’s a vertical scaling maximum threshold you must set.

How can RDS read performance be improved?
You can leverage read replicas which are eventually consistent. This way of horizontal scaling will improve the performance.

Does it cost to have replication network traffic across AZs?
For RDS the answer is no, since it is a managed service they don't charge you the replication traffic across AZs in the same region, different from other services which do charge cross AZs traffic. Though cross-region replication IS charged, it has a network cost.

How can I shield my RDS from disasters?
You can enable Multi-AZ which is a SYNC replication to a standby instance from the master. This enabling operation will cost no-time since the master instance creates a DB Snapshot, then initiates the replica and syncs it under the hood. This property is for disaster recovery, once there’s a problem in the master instance, the stand-by instance becomes the new master. Read replicas can also be set up as MultiAZ.

Is it possible to encrypt RDS data?
Yes it can be done at rest by configuring it at launch time for the master instance as well as for the read instance, but the master instance must be encrypted if the read instances are planned to be as well.  Also it can be at flight using SSL on either MySQL or PostgreSQL.

Can I encrypt an RDS after launch time and without doing it at flight?
Yes, similar to what you do when encrypting EBS volumes, you take a snapshot of the database, copy the snapshot into an encrypted one, restore a database from the encrypted snapshot, and migrate the data from the unencrypted to the encrypted database.

Is there something that can offer better performance for databases?
Yes, Aurora can offer x5 performance on MySQL and x3 on PostgreSQL. It cost 20% more than RDS but it’s more efficient and it replicated by default to 3 AZs both on read instances and write instances. Since read and write instances are separated, there’s two endpoints to handle those operations. The writer endpoint requests the master instance for writing. The reader endpoint requests a load balancer which in turn requests to the read replicas which can be up to 15. In case of failure any of the read replicas can become the master instance for write. Aurora also supports cross region replication. It auto scales in 10GBs util 128TB.

Can Aurora be encrypted?
Yes, the Aurora service has the same security features as the other RDS databases.

Can Aurora scale horizontally?
Yes, you can scale your read replicas and the reader endpoint will extend to use the new instances. Also if you can create custom endpoints for read so that you use different subsets of the read replicas instances. You can also have multi master where multiple nodes make read and write operations.

Is there a way to use Aurora without provisioning capacity?
Yes with Aurora serverless where instantiation and scaling is automatic and it’s based on the workload. It doesn’t require capacity planning and it’s pay for use so it can be cost effective.

How can I obtain more accurate data making queries to Aurora?
Aurora uses ML services like SageMaker and Comprehend so a query to Aurora can obtain information from the ML services which gives you more accurate information in a certain situation.

Can I use Aurora cross-region?
You can have read replicas cross-region. Or you can have Aurora Global in which you set 1 primary region and up to 5 secondary regions where each can have up to 16 read replicas. Promoting to a secondary region takes at most 1 minute.

What cache technology can I use to ease database loads?
You can use ElasticCache which works with two technologies, Redis and Memcached. The first one Redis supports Multi-AZ as in RDS, also has AOF persistence and brings high availability. It can also be backed up and restored. Memcahed is purely a cache alternative with no persistence of backup of data.

What's the security provided when using ElasticCache?
If you are using Redis, you can use Redis AUTH with user and password authentication or SSL encryption at flight. If you are using Memcached otherwise, you can use SASL authentication.


Section 10. Route 53

What does the process look like when making a request to a DNS?
First the local DNS servers looks for the DNS, then it ask the Root DNS server, with their IP response the local server asks the TLD (Top level domain) and with it’s IP response the local server asks the SLD(Second level domain) which is the domain registrar service for the domain and should deliver the appropriate IP to the web server.

How can I set up a DNS server?
You can use Route53 which is an scalable managed service with the feature of being the only SLA service on AWS. This one also serves as a DNS registrar. With the service you must pay 0.5USD for each hosted zone and a minimum of ~12USD per year for the domain.

What types of hosted zones are in Route53?
There's public and private hosted zone, where in a public hosted zone traffic from the internet can access your services being mapped by the domain. Whereas in the private hosted zone only entities from inside the same VPC can access it.

What types of record types does Route53 support?
There’s A which maps to IPv4, AAAA which maps to IPv6, CNAME which maps to other hostnames that map themselves to A or AAAA, and there’s NS which maps to named servers on the hosted zone. There’s also other supported types but these are the main ones.

How can reduce traffic on Route53 without reducing the actual number of visits?
You can define a high TTL(Time to live) since it’s the time the response with the IP for a DNS will be cached. Since a request is cached then we would be reducing the traffic on Route53.

Can I point in some way to root domains in Route53?
Since CNAME record types are the record type to point to hostname and they only allow pointing to NO root domains they don't work for that purpose. There is a way and it’s using Alias where it points to AWS Resources and allows you to point to root domains. On alias you cannot set TTL since it’s set automatically by AWS. It also automatically recognizes changes in resources IPs. It also evaluates health.

What targets can Alias on Route53 have?
You can point to Route53 records that are in the same host zone, but can also route to services S3, ELB, CloudFront, VPC, API Gateway, Global accelerator and Elastic Beanstalk.

Can I specify how Route53 should respond with different IPs according to different criteria?
Yes you can by specifying the Routing Policy for a record, this can be of various types which will dictate how the IPs will be returned.

What types are there for Routing Policy on Route53?
There’s Simple where you can specify multiple IP values and the client will choose any of them randomly, here there will be no health checks. Latency which will return IPs based on the latency of the different records IPs with the same record name in different regions. Weighted where you specify different records with the same record name and each with different weights, and these weights will guide how much traffic will go into each record. Failover which retrieves records IPs based on the health checks being healthy for the primary and if unhealthy for the secondary record. Geolocation which returns IPs based on the location that could be specific continents or countries or default for the rest of not specified areas. Geoproximity is similar to geolocation but instead of specifying a specific place, you specify locations and the bias which would be the measure of how extensive the area from that location should  be to redirect clients to it. Multi-Values is similar to Simple but instead of defining multiple values in one record, you define multiple records with the same record name and one value and it will return multiple values as in Simple. The big difference here is that multi-values records support health checks.
How can health checks be defined on Route53?
There are 3 kinds of ways you can define health checks and they are for Endpoint where you define a health check for an IP, and there will be 15 health checks around the world for this endpoint. Calculated where you define health checks based on the result of other health checks and CloudWatch Alarm where you can determine a health check result based on a CloudWatch Alarm.

How can I define a health check (that's coming from all over the world) for a private hosted zone endpoint?
You can activate a CloudWatch alarm based on the status of the endpoint and then create a CloudWatch Alarm healthcheck.

Can I buy a domain in a different Domain Registrar than AWS one and still use the DNS service with it?
Yes you just have to specify the domain namespaces on the AWS DNS service.


Section 11: Classic Solutions Architecture Discussion

How can I as a developer only worry about the code when deploying an application?
You can use ElasticBeanStalk which is a managed service that takes care of the infrastructure that powers your application. This service handles automatic provisioning of resources and therefore it scales. It uses services such as EC2, ELB, RDS, ASG. The services itself it’s free but you will be charged for the underlying used resources.

How’s ElasticBeanstalk structured?
You can have multiple applications which have versions and environments. A version of an application has its environment configuration. An environment consists of the resources being used by the application and there can be different tiers of an environment. There are 4 phases for an application on elasticbeanstalk.

What are the different tiers on an ElasticBeanstalk environment?
It can be either WebServer or Worker. For WebServer the architecture consists of an autoscaling group in different AZs and a load balancer for that ASG. And the Worker has an SQS Queue which is pulled by instances in different AZs in an ASG. These two different tier environments can work together.

What are the phases of an application on ElasticBeanstalk?
First the application is created, then the version of the application is updated, the environment is launched and the configuration is managed. While managing the configuration you can update the version and that will loop back to the second phase.



Section 12: S3 Introduction

What's the biggest size you can store in S3?
You can upload up to 5TB objects on a bucket but the maximum size to upload in a fly is 5GB, if you are uploading objects bigger than that, then you need to use multi-upload.

How can you prevent you wrongfully delete a file?
You can enable versioning where objects get a version ID and when deleted are not permanently deleted but instead they get a deleted marker. When versioning is disabled objects get replaced by new versions instead of persisting them.

Can you encrypt objects on S3?
Yes you can define the encryption of an object when uploading it or you could even define a default encryption type for a bucket which will determine the description of all the bucket’s uploaded objects. There are different types of encrypting an object. There’s SSE-S3 which works with the S3 encryption key. There’s SSE-KMS which uses a KMS key for encryption. And lastly there is SSE-C which is intended for providing your own encryption key, and even though all types up to this point are server side, you are responsible for providing the key when uploading and also decrypting the data, S3 provides the encryption process. Now there’s a four type and it is to not use S3 server side encryption but instead upload your encrypted files, this would be a client side encryption.

How can you add security for an S3 bucket apart from setting the policies?
There’s the possibility to block all public access by extra settings. This is another layer of security and it acts on top of the defined policies. You can define it both for an object and for the object’s bucket, the latter for blocking all public access of the bucket’s objects.

Can the internet receive my static website once setting it on S3?
You first need to make it public by disabling the block all public access configuration and also setting the appropriate policy.

Why requesting an object from a different bucket static website may fail?
It may be because you haven't set the appropriate CORS policies so even though you can access it from anywhere, the CORS policy is going to fail in the static website.

How is S3 consistency?
It’s strongly consistent since December 2020.


Section 13: AWS SDK, IAM & Policies

How can I get information about an instance without needing proper IAM permissions?
You can get the information by going to the http://169.254.169.254/latest/meta-data endpoint from inside the EC2 instance.
Section 14: Advances S3 & Athena

How can I secure the permanent deletion of objects on S3?
You can activate MFA after enabling versioning. This MFA can't be enabled from the AWS console, so you will have to use the AWS CLI.

How can I log activity from my S3 bucket?
You can use S3 Access logging to store logs on S3 buckets. Configuring server access logging on the logging bucket with the target of the bucket to be logged.

Can I replicate deletes when replicating S3 buckets?
You may set the configuration of replicating the delete markers which is disabled by default, but there is no way to replicate permanent deletes.

Can a S3 bucket be replicated into another region?
Yes S3 bucket replication can be either CRR (Cross region replication) or SRR (Same region replication). The first is intended for compliance, lower latency access. And the latter is intended for log aggregation or live replication between production and test environments.

Does the data before enabling replication get replicated?
No, but you can do S3 batch replication to replicate existent objects or failed replication ones.

Are there tiers on S3?
Yes you can choose objects to be inside a specific storage class. There’s the Standard storage class which is assigned by default and it's intended for content distribution, big data or gaming since it has the best availability of 99.99%. The Standard Infrequent Access (IA) storage class which is intended for disaster recovery or backup since it’s the second must available 99.9% and at a lower cost than standard. There’s One Zone Infrequent Access which it’s intended for secondary backup and has 99.5% availability but it’s on only one AZ. Keeping lowering pricing follows Glacier storage classes with lower cost and also retrieval cost, intended for archiving and backup, there are 3 types of glacier and they differ in retrieval time. There’s Glacier Instance Access which takes milliseconds of retrieval, then Glacier Flexible Retrieval which has expedited (1 to 5 minutes), standard (3 to 5 hours), or bulk (5 to 12 hours), and Glacier Deep Archive with standard (12 hours) and bulk (48 hours). Finally there is Intelligent tiering which is automatically assigned and it adjusts by the frequency of accessing the bucket, it doesn't have retrieval cost but has a monitoring cost.

What are the types Intelligent tiering adjusts to?
Similar to the storage classes, it defaults to Frequent Access, then if a file is not accessed in 30 days it goes to Infrequent Access, 90 days to Archive Instant Access, 90 to 700+ days to Archive Access or 180 to 700+ Deep Archive Access.



What is the durability of S3?
It’s assured to be 11 9s across different AZs. That means 99.999999999% which corresponds to a loss of one file out of 10 million uploaded, and every 10 thousand years. In other words, it's very durable.

Can I specify what storage class objects of a bucket will be in automatically without using intelligent tiering?
You can set lifecycle rules for all or some objects of the bucket and specify the transitions depending on the age of the objects.

What type of lifecycle rules can you specify on S3?
You can define either transition or expiration rules. And rules can reference prefixes or tags.

Is there any help to know when to transition objects on S3?
Yes, but only for transitions between standard and standard IA and it’s called S3 Analytics, it helps you determine how much time you should wait to transition the objects.

What’s the performance of S3?
It scales automatically to a high rate of requests to 3500 for PUT, POST, DELETE, COPY and 5500 for GET, HEAD per second per prefix. You can also use multi-part upload which uploads parts of the objects in parallel. Alternatively to improve performance when uploading to a far away location you can use S3 Transfer Acceleration where AWS uploads to an edge location from which point it will transfer via private network faster to the far away location. Another way to improve performance when downloading is by using Byte-Range fetches where you download only parts of an object and you can also download them all in parallel.

What can limit S3 performance?
The use of SSE-KMS encryption can limit the performance since S3 must request the API of KMS which has its quota per second requests.

Is there a way to filter information from objects of S3 without downloading them whole?
You can use S3 select and Glacier select where you use SQL queries to filter rows and columns of data inside objects and it returns you only the information that you need which can represent 400% faster and 80% cheaper.

How can I act on actions performed on S3?
You can take advantage of event notification and connect your events to be sent to either SNS, SQS or Lambda. You could also opt to not specify it manually and send them to event bridge.

How can one ease up costs on S3 when users download large size data?
You can setup requester pays which charges the users for downloading the data instead of you, but the user must be an authenticated aws user to be billed.

What can I use for advanced querying s3 data for example querying logs?
You can use Athena which is a serverless service that uses SQL and it's really powerful to analyze data. It charges per TB scanned. It’s intended for use cases like reporting, business intelligence, analytics, etc.

Is there any way to implement locks to data for compliance purposes?
Yes, you can use Object Lock for S3 or Vault lock for Glacier which follow a Write Once Read Many WORM model. With this objects are locked by a policy and the policy itself is also locked. Object lock it’s for object deletion and you can set either a retention period or a legal hold which does not have expiry date, here you can also set a mode of either governance where only a user can edit or compliance where not even the root user can edit.


Section 15: Cloudfront & Global Accelerator

How can I access certain resources in far away regions with better performance?
You can use CloudFront which has edge locations around the world and responds from near locations. It can catch files to deliver them faster.

What kind of resources can CloudFront access?
It can access S3 buckets and Custom HTTP.

How does CloudFront work?
It receives requests, looks them up in the cache and if there is no cached response it makes the request to the origin, stores the response in cache based on the configurations and returns the response to the client.

How does cloudfront allow connection from S3?
With origin access identity OAI you can access S3 without it being public.

How to restrict traffic coming to CloudFront?
You can do a whitelisting or blacklisting of countries that can access it.

When to use CloudFront instead of using S3 CRR?
It’s true that you can replicate your buckets to different regions to improve latency of retrieval for clients, but you would use CloudFront when you need availability of static content cached everywhere. When you need to provide dynamic content with low latency you can opt to replicate cross region.

Can I exclusively restrict CloudFront urls?
Yes, you can use Signed URL or Signed Cookies which provides users with access with expiration time and can restrict access to certain IP ranges, path, date. The difference between signed url and signed cookie is that the first is intended to link one file, and the cookies one is intended to link to multiple files.

When should I use CloudFront Signed URL/Cookies instead of S3 Presigned URL?
Cloudfront on top of giving restrictions like IP, you can leverage caching features. Also you can define any path as a Signed URL/Cookie and not just objects, so you should use it when you have those requirements. Now the S3 presigned url user has the same permissions as the IAM user that created it, you should use it if you are granting direct access to S3 and are not managing your flows with CloudFront.

How does pricing work in CloudFront?
There’s a variation in the cost depending on the region. For this there’s different Price Classes you can define, there’s the All class which uses all edge locations therefore the best performance and the most expensive. There’s the 100 class that uses almost all edge locations (not using south america and australia)(don't use the most expensive) and there’s the 200 class that uses only a few locations (america and europe).

What if I have different origins to be requested?
You can use multi origin, with which depending on the path you can define multiple request destinations (origins).

How can I handle failing origins in requests?
You can use Origin groups inside which you define a primary and secondary origin for failover and therefore maximize availability.

How can I increase traffic security in CloudFront?
You can leverage Field Level Encryption where CloudFront uses asymmetric encryptions and encrypts defined up to 10 fields to be decrypted when arrived at the origin with an specific private key.

Is there an alternative to improve performance when requesting far away locations?
You can use Global Accelerator service. It works by using Anycast(same IP for all hops so takes the nearest one) and traffic goes to the nearest close location and then to the origin. It lowers latency and doesn’t cache data, just improves performance when requesting origins. It has health checks of origins for failover, and gets security for Ddos protection from AWS Shield as CloudFront. This Global Accelerator is intended to be used when you need connections without catching and non-HTTP cases like gaming.


Section 16: Storage Extras

Is there a faster way to transfer files in and out of AWS when the sizes are considerable (TBs, PBs or even EBs)?
Yes you can use the Snow Family which are devices sent by AWS to transfer files locally without the use of a network and ship them back to AWS facilities so they can include them in their infrastructure (S3). It’s important to note that these files transfer can be in or out AWS. These devices are used for what is called data migration and edge computing (Operating cloud locally without internet connection or computing power).

What devices are there in the snow family?
There’s the Snowball edge which can receive at most 80TBs, there’s the Snowcone which is smaller and receives up to 8TBs but it’s more portable and resistant and also supports network traffic for transfering to AWS. And there’s the Snowmobile which are trucks sent by AWS and can transfer EBs, has up to 100PB to store and transfer in parallel.

What are the characteristics of the snow devices?
The Snowball edge can be either compute optimized with 42TB storage, 200+GB RAM and 52vCPUs, while storage optimized has 80TB storage, 80GB RAM and 40vCPUs. The Snowcone has 8TB storage, 4GB RAM, and 2CPUs. These devices can run EC2 instances and lambdas.

How can I use these devices?
You can use them by installing the software OpsHub from which you can run different AWS services.

Can I archive the data micrated with snowball?
Yes but not directly, all that data is being stored in S3 so you must create a lifecycle policy to migrate the coming data to Glacier.

What other high performance way is there to store files?
There is Amazona FSx which is a service that provides high performance 3rd party file systems like Windows File Server and Lustre. Both can be accessed directly with VPN or Direct Connect and can be defined a storage of SSD or HDD. The windows file server is a windows file system and supports the SMB protocol, can be defined on ec2 instances even if they are linux and can be multi-AZ and can scale up to 10GB/s and millions of IOPS. The Lustre is a linux file system used for large scaled computing like HPC, machine learning or video processing and can scale up to 100GB/s and millions of IOPS, it is also seamlessly integrated with S3 and it can be defined as one of two deployment options.

What deployment options can FSx Lustre be defined? 
There’s Scratch file system intended for temporal storage, lower cost and higher performance, it doest replicate to another server so server failure means loss of information. And there’s Persistence file system intended for long term storage with replication over the same AZ and failover replacement.

Is there a way to use cloud alongside on premises for storage?
You can use Storage Gateway in which there are 3 types of it. There’s File Gateway that is used for low latency cache where on-premises file gateway connect to s3 for object storage when using NFS and SMB protocol. There’s Volume Gateway intended for on-premises backup with EBS backed by S3 using iSCSI protocol; it can be either cached volumes for low latency retrieval or store volume. And there’s Tape Gateway intended for backup of virtual tapes for companies that use tapes processes. There is also a file gateway for FSx Windows file server from which you can leverage low latency local cache.

Do I always need to use an API for using S3 or EFS?
No, you can use FTP, FTPS or SFPT with the Transfer family service, which connects to the AWS services with IAM roles and you access them through those protocols. Also can manage users and use Active directory for authentication. It’s highly available, scalable and reliable and you pay per endpoint provisioned per hour plus data transferred in GBs. Can also be integrated with multiple authentication services.


